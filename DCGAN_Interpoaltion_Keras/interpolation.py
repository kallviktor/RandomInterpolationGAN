import numpy as np
from numpy import zeros, ones, arange, tile, sum
from numpy.random import randint, choice
from collections import Counter
from interpolations_help_fcns import *
from math import dist

def stochasticSMC_interpol(generator, discriminator, DoG, config,z0=None,zT=None):
    
    """
    
    This function performs interpolation between a pair of codes living in the latent space. The interpolation is stochastic,
    meaning the interpolation is generated by a Gaussian bridge (i.e. a multivariate normal sample conditioned on start and
    end positions). Further, the interpolation utilizes a particle filter that forces the trajectory to move along codes that
    represent realistic samples from the data distribution (in this case images of handwritten digits). The weight function, which
    determines a particle's chance of survival, is given by the discriminator network (a.k.a. 'critic') combined with the
    generator network (a.k.a. 'artist') such that a code may flow through the artist, then the output of which flows through the
    critic and finally becomes a score between 0 and 1. The higher the score, the higher the chance of survival. In this way the
    interpolation is forced to produce a realistic sequence of images in the pixel space.
    
    """
    
    # Input arguments ==============================================================================
    # The 'generator' is a generator network of a GAN (generative adversarial network), here a keras
    # sequential object.
    # The 'discriminator' is a discriminator network of a GAN, also a keras sequential object.
    # The 'DoG' is the combined network of 'generator' and 'discriminator', indeed 'discriminator'
    # composed with 'generator'.
    # The 'config' is the setup of the program set by the user elsewhere.
    
    # Setup ========================================================================================
    # zDim is the dimension of the latent space, data type int64
    # z0 is starting position of interpolation (at time 0), data type numpy.array shape = (zDim, 1)
    # zT is ending position of interpolation (at time T), data type numpy.array shape = (zDim, 1)
    # T is elapsed total time, a tuning paramter essentially, data type int64
    # N is number of visited points of interpolation, including start and end point, data type int64
    # n_parts is size of particle filter, i.e. number of particles, data type int64
    
    if not config.metrics:
        print_interpolation_initialized()

    zDim = config.z_dim

    #z0 = array([[0.5],[-1.5]])
    #zT = array([[-1.5],[1]])
    """
    z0=array([[-1.39692937],
        [-0.25102801],
        [0.90856929],
        [0.59829457],
        [0.76654908],
        [-1.04248133],
        [-0.30277766],
        [0.90546642],
        [0.51776581],
        [-0.44183417]])

    zT=array([[-0.42141017],
        [2.0301715 ],
        [1.08471715],
        [-1.57596246],
        [-1.23363746],
        [0.27332751],
        [-0.54345123],
        [0.71440436],
        [1.02717621],
        [-0.60785543]])
    
    """

    if z0.size == 0:
        z0 = get_valid_code(DoG, config)
    if zT.size == 0:
        zT = get_valid_code(DoG, config)

    #print('z0:   ', z0)
    #print('zT:   ', zT)
    # z0 = ones((config.z_dim,1))*config.z_start
    # zT  = ones((config.z_dim,1))*config.z_end
    
    T = config.int_time
    N = config.int_steps
    n_parts = config.nmrParts
    
    # Assemble / allocate matrices ====================================================================================
    # weights_all is a matrix where the i:th row, j:th column represents the i:th particle's weight at
    # its j:th step, data type numpy.array shape = (n_parts, N-2)
    # S is a list of indices (from 0 up to (n_parts-1)) for each particle, data type numpy.array shape = (n_parts, )
    # PartsPaths is a 3-D tensor where the i:th sheet represents the i:th particle's entire trajectory from start at z0
    # finish at zT. The j:th row is the j:th coordinate's (there are zDim coordinates) trajectory, and the k:th column
    # is the k:th position of (out of N positions) the particle path, data type numpy.array shape = (n_parts, zDim, N)
    
    weights_all = zeros((n_parts, N-2))
    S = arange(n_parts)
    
    PartsPaths = zeros((1, zDim, N))
    PartsPaths[0,:,0] = z0.reshape(-1)
    PartsPaths[0,:,-1] = zT.reshape(-1)
    PartsPaths = tile(PartsPaths, (n_parts, 1, 1))
    
    # Run program ========================================================================================================
    # dt is the time step used when sampling a Gaussian bridge, data type float64
    # S_re is a list of resampled particle indices, used in the particle filter. It is initialized as a list of zeros,
    # hence pointing at particle with index 0 (note: this does not matter since all particle start at z0), data type
    # numpy.array shape = (n_parts, )
    # weights is a list of weights for each particle's next trajectory step (the i:th element of weights is the associated
    # weight to the particle with index i), data type numpy.array shape = (n_parts, )
    
    #dt = T / N
    S_re = randint(0, 1, n_parts)
    weights = zeros(n_parts)
    
    t = array([linspace(0,T,N)])
    smat = tile(t, (N, 1))
    tmat = smat.transpose(1,0)

    Mean_Matrix = muhat_mean(t,z0,zT,T).T
    Cov_Matrix = khat_cov(smat, tmat, T)


    #print(Cov_Matrix)
    #print(Mean_Matrix)

    PartsPaths_Temp = PartsPaths

    for step in range(N-1):
        
        if not config.metrics:
            print_interpolation_progress(N,step)
        
        # Outer for-loop ==================================================================================================
        # The idea is to generate n_parts Gaussian bridges between z0 and zT where z0 (the starting position) is updated
        # at the end of every loop. Indeed, the Gaussian bridges' duration / time length and number of time steps decreases
        # linearly for every loop such that the time step is held constant.
        
        # Tcur is the remaining time, data type float64
        # Ncur is the remaining number of time steps that should be taken, data type int64
        # parts is a 3-D tensor that stores the generated Gaussian bridges for each of the n_parts particles. Note: number of
        # columns (= Ncur) decreases with each loop, data type numpy.array shape = (n_parts, zDim, Ncur)
        
        #Tcur = T - dt * step
        #Ncur = N - step
        #parts = zeros((n_parts, zDim, Ncur))
        
        # surv_freq, short for 'survivor-frequency', is a dictionary constructed from S_re where each key is an index (survivor
        # of the resampling step) and the corresponding value is the frequency that index appeared in S_re, data type python
        # dictionary object
        # resampled is a list of 'survivor' indices, data type numpy.array shape = (_, )
        # freq is a list of frequencies for resampled, data type numpy.array shape = (_, )
        
        surv_freq = Counter(S_re)
        resampled = list(surv_freq.keys())
        freq = list(surv_freq.values())
        
        # First inner for-loop ================================================================================================
        # Here we evolve the particles according to their new positions (after the resampling step below). The new positions
        # are the updated z0 as explained above. Note that the particles may have different z0 which is why iteration over all
        # different such z0 is necessary. Further, observe that several particles may have the same z0, which is also taken into
        # consideration.
        # The BPvec function (short for Bridge-Process-vectorized) outputs f Gaussian bridges with start and end points z0 and
        # zT, respectively. The time constraint is set by Tcur and number of steps by Ncur.
        
        # pointer is just keeping track of how many particles we have evolved so far, data type int64
        # nmr_res is the number of unique resampled particle indices, data type int64
        
        pointer = 0
        nmr_res = len(resampled)
        



        Sub_Cov_Joint = NewjointCov(Cov_Matrix, step+1)
        Sub_Mean_Joint = NewjointMean(Mean_Matrix, step+1, zDim)

        #print(Sub_Cov_Joint)
        #print(Sub_Mean_Joint.shape)
        

        Sub_Cov_Cond = CovConditional(Sub_Cov_Joint,zDim)

        #print(Sub_Cov_Cond)

        

        for i in range(nmr_res):

            res_idx = resampled[i]
            f = freq[i]
            
            path = PartsPaths[res_idx,:,:]
            
            if step == 0:
                Sub_Mean_Cond = Sub_Mean_Joint[0,:].T
                #print(Sub_Mean_Cond.shape)
            else:
                Sub_Mean_Cond = MeanConditional(Sub_Mean_Joint,Sub_Cov_Joint,path.T,step+1,zDim)

            #print(Sub_Mean_Cond)
            
            
            BatchParts = multivariate_normal(Sub_Mean_Cond, cov=Sub_Cov_Cond, size=f)

            #print('f: ',f)
            #print('Batch: ',BatchParts)

            #BatchParts = BatchParts.reshape((f,zDim))
            #print('') 
            #print(BatchParts.shape)
            #print(PartsPaths.shape)

            PartsPaths_Temp[pointer:pointer+f,:,:] = path
            PartsPaths_Temp[pointer:pointer+f,:,step+1] = BatchParts

            #PartsPaths[pointer:pointer+f,:,step+1] = BatchParts

            """
            print(2*'\n')
            print('paths: ',PartsPaths)
            print('\n')
            print('batch: ',BatchParts)
            """

            #z0_curr = PartsPaths[res_idx,:,step].reshape(zDim,-1)
            #parts[pointer:pointer+f,:,:] = BPvec(z0_curr, zT, Tcur, Ncur, f)
            
            pointer += f

        PartsPaths = PartsPaths_Temp

        #print('parts: ',PartsPaths)
        # Second inner for-loop ================================================================================================
        # Here we perform the resampling step of the particle filter. Note that the 3-D tensor parts contains proposed
        # particle paths, however only the second position of each particle path, i.e. the second column of each sheet in parts,
        # is of interest (since we evolve the particle filter stepwise). Each such proposed next-step is given a weight assigned
        # by the function weight_func (which in essence is the combined DoG network).

        z = PartsPaths[:,:,step+1]

        weights = weight_func(z, zDim, DoG)
        
        # Normalize weights
        weights = weights / sum(weights)
        
        # Resampling
        S_re = choice(S, n_parts, replace=True, p=weights)
        
        # Storing weights for each new position of every particle path
        #weights_all[:,step] = weights[S_re]

        # Update particle paths
        PartsPaths = PartsPaths[S_re,:,:]
    
    if not config.metrics:
        print_interpolation_complete()
    
    
    # Return the interpolation ============================================================================================
    # The interpolation that will be returned is a sequence of the positions with highest weigthts for every step over all
    # the particles. In other words, for the j:th column in weights_all we select the row index i with largest compoent / 
    # weight, implying that the j:th column of the i:th sheet of PartsPaths will be selected as the j:th position in the
    # interpolation.
    # The explicit function simply returns indices corresponding to the maximum value of a list.
    
    # interpol is initialized as 0:th sheet of PartsPaths (this way the interpolation contains the correct start and end points),
    # data type numpy.array shape = (zDim, N)
    rand_path = randint(n_parts)
    interpol = PartsPaths[rand_path,:,:]
    
    """
    for step in range(N-2):
        
        idxs, _ = explicit(weights_all[:,step])
        idx = idxs[0][0]
        
        interpol[:,step+1] = PartsPaths[idx,:,step+1]
    """
    #PartsPaths[0,:,:]

    """
    
    numParts = PartsPaths.shape[0]
    numSteps = PartsPaths.shape[2] - 1

    min_max_step = []
    for i in range(numParts):
        steps = []
        for j in range(numSteps):
            steplength = dist(PartsPaths[i,:,j],PartsPaths[i,:,j+1])
            steps.append(steplength)
        max_step = max(steps)
        min_max_step.append(max_step)

    #idxs, _ = explicit(min_max_step)
    #idx = idxs[0][0]
    idx = min_max_step.index(min(min_max_step))   
    interpol = PartsPaths[idx,:,:]
    """
    """

    numParts = PartsPaths.shape[0]
    numSteps = PartsPaths.shape[2] - 1

    interpol = PartsPaths[0,:,:]
    curpoint = interpol[:,-1]

    for i in range(numSteps,1,-1):
        props = []
        for j in range(numParts):
            stepsize = dist(curpoint,PartsPaths[j,:,i-1])
            props.append(stepsize)

        bestP = props.index(min(props))
        interpol[:,numSteps-1] = PartsPaths[bestP,:,i-1]
        curpoint = interpol[:,numSteps-1]
    """
    #print(interpol)
    #print(PartsPaths[1,:,:])
    #print(PartsPaths.shape)
    #print(interpol)
    #print(z0)
    print(zT)
    print(interpol)
    return interpol,z0,zT

def linear_interpol(config,z0,zT):
    # Linear interpolation

    if z0.size == 0:
        z0 = normal(0, 1, size=(1,config.z_dim)).T
    if zT.size == 0:
        zT = normal(0, 1, size=(1,config.z_dim)).T

    """
    start=array([[-1.39692937],
        [-0.25102801],
        [0.90856929],
        [0.59829457],
        [0.76654908],
        [-1.04248133],
        [-0.30277766],
        [0.90546642],
        [0.51776581],
        [-0.44183417]])

    end=array([[-0.42141017],
        [2.0301715 ],
        [1.08471715],
        [-1.57596246],
        [-1.23363746],
        [0.27332751],
        [-0.54345123],
        [0.71440436],
        [1.02717621],
        [-0.60785543]])
    """
    t     = array(linspace(0, 1, config.int_steps)).reshape(1,-1)
    z_seq = zT * t + (1 - t) * z0
    x = z_seq[0,:]
    y = z_seq[1,:]

    #print(z_seq)
    return z_seq,z0,zT

def stochastic_interpol(generator, discriminator, DoG, config,z0,zT):


    if z0.size == 0:
        z0 = get_valid_code(DoG, config)
    if zT.size == 0:
        zT = get_valid_code(DoG, config)

    T = config.int_time
    N = config.int_steps
    n_parts = 1

    interpol = BPvec(z0, zT, T, N, n_parts)
    print(zT)
    return interpol[0,:,:],z0,zT


