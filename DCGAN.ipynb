{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNT3NVBh1jlkZ7DuRP+UQYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kallviktor/RandomInterpolationGAN/blob/RewriteDCGAN/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C12B_Jtthu2r"
      },
      "source": [
        "https://github.com/kmualim/DCGAN-Keras-Implementation/blob/master/dcgan-mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phK8FR8Vfwlq",
        "outputId": "1f68dfce-802c-4772-8bdc-97441b63941e"
      },
      "source": [
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Reshape \r\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D \r\n",
        "from keras.layers import LeakyReLU, Dropout \r\n",
        "from keras.layers import BatchNormalization \r\n",
        "from keras.optimizers import Adam, RMSprop\r\n",
        "from keras.initializers import RandomNormal, Zeros\r\n",
        "from keras.datasets import mnist\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import sys \r\n",
        "import numpy as np\r\n",
        "#Model1 = GAN(100)\r\n",
        "class GAN(object): \r\n",
        "    def __init__(self, input_latentDim):\r\n",
        "      self.img_rows = 28 \r\n",
        "      self.img_cols = 28 \r\n",
        "      self.channel=1\r\n",
        "      self.img_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "      self.latentDim = input_latentDim\r\n",
        "      optimizer = Adam(0.0002, 0.5)\r\n",
        "     \r\n",
        "    def build_discriminator(self):\r\n",
        "      discriminator = Sequential()\r\n",
        "      depth = 32 \r\n",
        "      dropout=0.25 \r\n",
        "      input_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "      \r\n",
        "      discriminator.add(Conv2D(depth*1, 3, strides=2, input_shape=input_shape, padding='same', kernel_initializer='random_uniform'))\r\n",
        "      discriminator.add(BatchNormalization(momentum=0.9))\r\n",
        "      discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "      discriminator.add(Dropout(dropout))\r\n",
        "      discriminator.add(Conv2D(depth*2, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      discriminator.add(BatchNormalization(momentum=0.9))\r\n",
        "      discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "      discriminator.add(Dropout(dropout))\r\n",
        "      discriminator.add(Conv2D(depth*4, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      discriminator.add(BatchNormalization(momentum=0.9))\r\n",
        "      discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "      discriminator.add(Dropout(dropout))\r\n",
        "      discriminator.add(Conv2D(depth*8, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      discriminator.add(BatchNormalization(momentum=0.9))\r\n",
        "      discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "      discriminator.add(Dropout(dropout))\r\n",
        "\r\n",
        "      # Each MNIST input = 28 X 28 X 1, depth = 1\r\n",
        "      # Each Output = 14 X 14 X 1, depth = 64 \r\n",
        "      # Model has 4 convolutional layer, each with a dropout layer in between \r\n",
        "\r\n",
        "      # Output \r\n",
        "      discriminator.add(Flatten())\r\n",
        "      discriminator.add(Dense(1))\r\n",
        "      discriminator.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "      img = Input(shape=(self.img_shape))\r\n",
        "      validity = discriminator(img)\r\n",
        "\r\n",
        "      D = Model(img, validity)\r\n",
        "      D.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\r\n",
        "      \r\n",
        "      return D\r\n",
        "\r\n",
        "    # generator takes noise as input and generates imgs\r\n",
        "                \r\n",
        "    def build_generator(self):\r\n",
        "      generator = Sequential() \r\n",
        "      depth = 128\r\n",
        "      dim = 7\r\n",
        "\r\n",
        "      # In: 100 \r\n",
        "      # Out: dim X dim X depth \r\n",
        "\r\n",
        "      generator.add(Dense(dim*dim*depth, input_dim=self.latentDim))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "      generator.add(Reshape((dim, dim, depth)))\r\n",
        "      generator.add(UpSampling2D())\r\n",
        "\r\n",
        "      # In: dim X dim X depth\r\n",
        "      # Out: 2*dim X 2*dim X depth/2 \r\n",
        "\r\n",
        "      generator.add(Conv2D(depth, 3, padding='same'))\r\n",
        "      generator.add(BatchNormalization(momentum=0.9))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "      generator.add(UpSampling2D())\r\n",
        "      generator.add(Conv2D(int(depth/2), 3, padding='same'))\r\n",
        "      generator.add(BatchNormalization(momentum=0.9))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "     \r\n",
        "      # Out : 28 X 28 X 1 grayscale image [0.0, 1.0] per pix\r\n",
        "      generator.add(Conv2D(1,3,padding='same'))\r\n",
        "      generator.add(Activation('tanh'))\r\n",
        "      \r\n",
        "      # noise = Input(shape=(100,))\r\n",
        "      noise = Input(shape=(self.latentDim,))\r\n",
        "      img = generator(noise)\r\n",
        "\r\n",
        "      G = Model(noise, img) \r\n",
        "      \r\n",
        "      return G\r\n",
        "\r\n",
        "class dcgan(GAN):\r\n",
        "  def __init__(self, input_latentDim):\r\n",
        "    super().__init__(input_latentDim)\r\n",
        "    # self.img_rows=28\r\n",
        "    # self.img_cols=28\r\n",
        "    # self.channels=1\r\n",
        "    # self.latentDim = input_latentDim\r\n",
        "\r\n",
        "    # building the generator \r\n",
        "    self.GAN = GAN(self.latentDim)\r\n",
        "    self.D = self.GAN.build_discriminator()\r\n",
        "    self.G = self.GAN.build_generator()\r\n",
        "\r\n",
        "    #z = Input(shape=(100,))\r\n",
        "    z = Input(shape=(10,))\r\n",
        "    img = self.G(z)\r\n",
        "    self.D.trainable = False\r\n",
        "    valid = self.D(img)\r\n",
        "    \r\n",
        "    self.combined = Model(z, valid)\r\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\r\n",
        "    \r\n",
        "    # training input \r\n",
        "    # To change dataset, place dataset below \r\n",
        "    (self.x_train, _), (_,_) = mnist.load_data()\r\n",
        "    self.x_train = self.x_train/127.5 -1.\r\n",
        "    self.x_train = np.expand_dims(self.x_train, axis=3) \r\n",
        "    #x_train = x_train/127.5 -1. \r\n",
        "    #x_train = np.expand_dims(x_train, axis=3)\r\n",
        "    self.n_samples = 25\r\n",
        "    # self.noise_dim = 100\r\n",
        "    self.noise_dim = 10\r\n",
        "  \r\n",
        "  # method to generate noise \r\n",
        "  def gennoise(self,batch_size, noise_dim): \r\n",
        "   \tx = np.random.normal(0, 1.0, (batch_size, self.noise_dim))\r\n",
        "   \treturn x\r\n",
        "\r\n",
        "  def plt_imgs(self,epoch): \r\n",
        "    noise = self.gennoise(self.n_samples, self.noise_dim)\r\n",
        "    fake_imgs = self.G.predict(noise)\r\n",
        "    fake_imgs = 0.5 * fake_imgs + 0.5\r\n",
        "  \r\n",
        "    fig,axs = plt.subplots(5,5)\r\n",
        "    count = 0 \r\n",
        "    for i in range(5): \r\n",
        "        for j in range(5): \r\n",
        "          axs[i,j].imshow(fake_imgs[count, :, :, 0], cmap='gray')\r\n",
        "          axs[i,j].axis('off')\r\n",
        "          count+=1\r\n",
        "      \r\n",
        "    fig.savefig(\"mnist_%d.png\" % epoch)\r\n",
        "    plt.close()\r\n",
        "  \r\n",
        "  \r\n",
        "  def train(self,n_epochs, batch_size):\r\n",
        "    train_hist={}\r\n",
        "    train_hist['D_losses']=[]\r\n",
        "    train_hist['G_losses']=[]\r\n",
        "    print(\"Start\")\r\n",
        "    true_labels=np.ones((batch_size,1))\r\n",
        "    gen_gene_labels=np.zeros((batch_size,1))\r\n",
        "    \r\n",
        "    for epoch in range(n_epochs):\r\n",
        "    \r\n",
        "      index = np.random.randint(0, self.x_train.shape[0], batch_size)\r\n",
        "      images = self.x_train[index]\r\n",
        "      \r\n",
        "      #noise_data = self.gennoise(batch_size, 100)\r\n",
        "      noise_data = self.gennoise(batch_size, 10)\r\n",
        "      gen_imgs = self.G.predict(noise_data)\r\n",
        "      \r\n",
        "      \r\n",
        "      d_loss = self.D.train_on_batch(images, true_labels)\r\n",
        "    \r\n",
        "      d_loss_generated = self.D.train_on_batch(gen_imgs, gen_gene_labels)\r\n",
        "      \r\n",
        "      total_d_loss = 0.5 * np.add(d_loss, d_loss_generated)\r\n",
        "      \r\n",
        "      train_hist['D_losses'].append(total_d_loss[0])\r\n",
        "        \r\n",
        "      #noise_data = self.gennoise(batch_size, 100)\r\n",
        "      noise_data = self.gennoise(batch_size, 10)\r\n",
        "      y1 = np.ones((batch_size, 1))    \r\n",
        "      \r\n",
        "      g_loss = self.combined.train_on_batch(noise_data, y1)\r\n",
        "\r\n",
        "      train_hist['G_losses'].append(g_loss)\r\n",
        "      print (' Epoch:{}, G_loss: {}, D_loss:{}'.format(epoch+1, g_loss, total_d_loss[0]))\r\n",
        "      \r\n",
        "      #if epoch%50==0:\r\n",
        "      if epoch%2 == 0:\r\n",
        "        self.plt_imgs(epoch)\r\n",
        "      \r\n",
        "    return train_hist\r\n",
        "\r\n",
        "#if __name__ == '__main__':\r\n",
        "latentDim = 10\r\n",
        "mnist_dcgan = dcgan(latentDim)\r\n",
        "train_hist = mnist_dcgan.train(10, batch_size=32)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            " Epoch:1, G_loss: 0.696929395198822, D_loss:0.8944361209869385\n",
            " Epoch:2, G_loss: 0.7193477153778076, D_loss:0.7503529489040375\n",
            " Epoch:3, G_loss: 0.7256073951721191, D_loss:0.5283777862787247\n",
            " Epoch:4, G_loss: 0.7433050870895386, D_loss:0.4568212032318115\n",
            " Epoch:5, G_loss: 0.7528917789459229, D_loss:0.29697130620479584\n",
            " Epoch:6, G_loss: 0.7478498220443726, D_loss:0.21806255728006363\n",
            " Epoch:7, G_loss: 0.7524862289428711, D_loss:0.1819745972752571\n",
            " Epoch:8, G_loss: 0.7617835998535156, D_loss:0.12767834961414337\n",
            " Epoch:9, G_loss: 0.7046108245849609, D_loss:0.14496906846761703\n",
            " Epoch:10, G_loss: 0.7010557651519775, D_loss:0.07853445038199425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hutkmn9C9byL",
        "outputId": "ca8f6923-92d2-40a9-b71f-c9a9f0b30f20"
      },
      "source": [
        "z = mnist_dcgan.gennoise(1,100)\r\n",
        "#z.shape\r\n",
        "x = mnist_dcgan.generator.predict(z)\r\n",
        "x.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJI8mFyEwVvc",
        "outputId": "86ac430d-4f96-4ada-86f0-ccb61a9b2c03"
      },
      "source": [
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Reshape \r\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D \r\n",
        "from keras.layers import LeakyReLU, Dropout \r\n",
        "from keras.layers import BatchNormalization \r\n",
        "from keras.optimizers import Adam, RMSprop\r\n",
        "from keras.initializers import RandomNormal, Zeros\r\n",
        "from keras.datasets import mnist\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import sys \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class GAN(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.img_rows = 28 \r\n",
        "    self.img_cols = 28 \r\n",
        "    self.channel=1\r\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "    self.optimizer = Adam(0.0002, 0.5)\r\n",
        "    # optimizer = Adam(0.0002, 0.5)\r\n",
        "    self.latentDim = 10\r\n",
        "  \r\n",
        "  def build_discriminator(self):\r\n",
        "    model = Sequential()\r\n",
        "    depth = 32 \r\n",
        "    dropout=0.25 \r\n",
        "    input_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "  \r\n",
        "    model.add(Conv2D(depth*1, 3, strides=2, input_shape=input_shape, padding='same', kernel_initializer='random_uniform'))\r\n",
        "    model.add(BatchNormalization(momentum=0.9))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    model.add(Conv2D(depth*2, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "    model.add(BatchNormalization(momentum=0.9))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    model.add(Conv2D(depth*4, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "    model.add(BatchNormalization(momentum=0.9))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    model.add(Conv2D(depth*8, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "    model.add(BatchNormalization(momentum=0.9))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "\r\n",
        "    # Each MNIST input = 28 X 28 X 1, depth = 1\r\n",
        "    # Each Output = 14 X 14 X 1, depth = 64 \r\n",
        "    # Model has 4 convolutional layer, each with a dropout layer in between \r\n",
        "\r\n",
        "    # Output \r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(1))\r\n",
        "    model.add(Activation('sigmoid'))\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    img = Input(shape=(self.img_shape))\r\n",
        "    validity = model(img)\r\n",
        "    \r\n",
        "    return Model(img, validity) \r\n",
        "\r\n",
        "    # generator takes noise as input and generates imgs\r\n",
        "  \r\n",
        "  \r\n",
        "  def build_generator(self):\r\n",
        "    generator = Sequential() \r\n",
        "    depth = 128\r\n",
        "    dim = 7\r\n",
        "\r\n",
        "    # In: 100 \r\n",
        "    # Out: dim X dim X depth \r\n",
        "\r\n",
        "    generator.add(Dense(dim*dim*depth, input_dim=self.latentDim))\r\n",
        "    generator.add(Activation('relu'))\r\n",
        "    generator.add(Reshape((dim, dim, depth)))\r\n",
        "    generator.add(UpSampling2D())\r\n",
        "\r\n",
        "    # In: dim X dim X depth\r\n",
        "    # Out: 2*dim X 2*dim X depth/2 \r\n",
        "\r\n",
        "    generator.add(Conv2D(depth, 3, padding='same'))\r\n",
        "    generator.add(BatchNormalization(momentum=0.9))\r\n",
        "    generator.add(Activation('relu'))\r\n",
        "    generator.add(UpSampling2D())\r\n",
        "    generator.add(Conv2D(int(depth/2), 3, padding='same'))\r\n",
        "    generator.add(BatchNormalization(momentum=0.9))\r\n",
        "    generator.add(Activation('relu'))\r\n",
        "\r\n",
        "    # Out : 28 X 28 X 1 grayscale image [0.0, 1.0] per pix\r\n",
        "    generator.add(Conv2D(1,3,padding='same'))\r\n",
        "    generator.add(Activation('tanh'))\r\n",
        "    \r\n",
        "    # noise = Input(shape=(100,))\r\n",
        "    noise = Input(shape=(self.latentDim,))\r\n",
        "    img = generator(noise)\r\n",
        "    \r\n",
        "    return Model(noise, img)\r\n",
        "    \r\n",
        "  # Build and compile discriminator\r\n",
        "  def DM(self):\r\n",
        "    # optimizer = Adam(0.0002, 0.5)\r\n",
        "    DM = self.build_discriminator() # = Model(validity,img)\r\n",
        "    DM.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\r\n",
        "    return DM    \r\n",
        "\r\n",
        "class dcgan(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.img_rows=28\r\n",
        "    self.img_cols=28\r\n",
        "    self.channels=1\r\n",
        "\r\n",
        "    # building the generator \r\n",
        "    self.GAN = GAN()\r\n",
        "    self.DM = self.GAN.DM()\r\n",
        "    self.generator = self.GAN.build_generator()\r\n",
        "\r\n",
        "\r\n",
        "    #z = Input(shape=(100,))\r\n",
        "    z = Input(shape=(10,))\r\n",
        "    img = self.generator(z)\r\n",
        "    self.DM.trainable = False\r\n",
        "    valid = self.DM(img)\r\n",
        "    \r\n",
        "    self.combined = Model(z, valid)\r\n",
        "    optimizer = Adam(0.0002, 0.5)\r\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n",
        "    \r\n",
        "    # training input \r\n",
        "    # To change dataset, place dataset below \r\n",
        "    (self.x_train, _), (_,_) = mnist.load_data()\r\n",
        "    self.x_train = self.x_train/127.5 -1.\r\n",
        "    self.x_train = np.expand_dims(self.x_train, axis=3) \r\n",
        "    #x_train = x_train/127.5 -1. \r\n",
        "    #x_train = np.expand_dims(x_train, axis=3)\r\n",
        "    self.n_samples = 25\r\n",
        "    # self.noise_dim = 100\r\n",
        "    self.noise_dim = 10\r\n",
        "  \r\n",
        "  # method to generate noise \r\n",
        "  def gennoise(self,batch_size, noise_dim): \r\n",
        "   \tx = np.random.normal(0, 1.0, (batch_size, self.noise_dim))\r\n",
        "   \treturn x\r\n",
        "\r\n",
        "  def plt_imgs(self,epoch): \r\n",
        "    noise = self.gennoise(self.n_samples, self.noise_dim)\r\n",
        "    fake_imgs = self.generator.predict(noise)\r\n",
        "    print(fake_imgs.shape)\r\n",
        "    fake_imgs = 0.5 * fake_imgs + 0.5\r\n",
        "  \r\n",
        "    fig,axs = plt.subplots(5,5)\r\n",
        "    count = 0 \r\n",
        "    for i in range(5): \r\n",
        "        for j in range(5): \r\n",
        "          axs[i,j].imshow(fake_imgs[count, :, :, 0], cmap='gray')\r\n",
        "          axs[i,j].axis('off')\r\n",
        "          count+=1\r\n",
        "      \r\n",
        "    fig.savefig(\"mnist_%d.png\" % epoch)\r\n",
        "    plt.close()\r\n",
        "  \r\n",
        "  \r\n",
        "  def train(self,n_epochs, batch_size):\r\n",
        "    train_hist={}\r\n",
        "    train_hist['D_losses']=[]\r\n",
        "    train_hist['G_losses']=[]\r\n",
        "    print(\"Start\")\r\n",
        "    true_labels=np.ones((batch_size,1))\r\n",
        "    gen_gene_labels=np.zeros((batch_size,1))\r\n",
        "    \r\n",
        "    for epoch in range(n_epochs):\r\n",
        "    \r\n",
        "      index = np.random.randint(0, self.x_train.shape[0], batch_size)\r\n",
        "      images = self.x_train[index]\r\n",
        "      \r\n",
        "      #noise_data = self.gennoise(batch_size, 100)\r\n",
        "      noise_data = self.gennoise(batch_size, 10)\r\n",
        "      gen_imgs = self.generator.predict(noise_data)\r\n",
        "      \r\n",
        "      \r\n",
        "      d_loss = self.DM.train_on_batch(images, true_labels)\r\n",
        "    \r\n",
        "      d_loss_generated = self.DM.train_on_batch(gen_imgs, gen_gene_labels)\r\n",
        "      \r\n",
        "      total_d_loss = 0.5 * np.add(d_loss, d_loss_generated)\r\n",
        "      \r\n",
        "      train_hist['D_losses'].append(total_d_loss[0])\r\n",
        "        \r\n",
        "      #noise_data = self.gennoise(batch_size, 100)\r\n",
        "      noise_data = self.gennoise(batch_size, 10)\r\n",
        "      y1 = np.ones((batch_size, 1))    \r\n",
        "      \r\n",
        "      g_loss = self.combined.train_on_batch(noise_data, y1)\r\n",
        "\r\n",
        "      train_hist['G_losses'].append(g_loss)\r\n",
        "      print (' Epoch:{}, G_loss: {}, D_loss:{}'.format(epoch+1, g_loss, total_d_loss[0]))\r\n",
        "      \r\n",
        "      #if epoch%50==0:\r\n",
        "      if epoch%2 == 0:\r\n",
        "        self.plt_imgs(epoch)\r\n",
        "      \r\n",
        "    return train_hist\r\n",
        "\r\n",
        "#if __name__ == '__main__': \r\n",
        "mnist_dcgan = dcgan()\r\n",
        "train_hist = mnist_dcgan.train(10, batch_size=32)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_95 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_56 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_57 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_58 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_59 (LeakyReLU)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 1025      \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 390,785\n",
            "Trainable params: 389,825\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_29 (Dense)             (None, 6272)              68992     \n",
            "_________________________________________________________________\n",
            "reshape_13 (Reshape)         (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_26 (UpSampling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_27 (UpSampling (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 291,713\n",
            "Trainable params: 291,329\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Start\n",
            " Epoch:1, G_loss: 0.6797021627426147, D_loss:0.9393770694732666\n",
            "(25, 28, 28, 1)\n",
            " Epoch:2, G_loss: 0.663440465927124, D_loss:0.6983038783073425\n",
            " Epoch:3, G_loss: 0.6507325172424316, D_loss:0.6379466950893402\n",
            "(25, 28, 28, 1)\n",
            " Epoch:4, G_loss: 0.6525896787643433, D_loss:0.6585943698883057\n",
            " Epoch:5, G_loss: 0.6357796788215637, D_loss:0.5179827213287354\n",
            "(25, 28, 28, 1)\n",
            " Epoch:6, G_loss: 0.6534567475318909, D_loss:0.4437680244445801\n",
            " Epoch:7, G_loss: 0.61332106590271, D_loss:0.40788571536540985\n",
            "(25, 28, 28, 1)\n",
            " Epoch:8, G_loss: 0.6397654414176941, D_loss:0.3886369615793228\n",
            " Epoch:9, G_loss: 0.6142491698265076, D_loss:0.3510808199644089\n",
            "(25, 28, 28, 1)\n",
            " Epoch:10, G_loss: 0.5481303930282593, D_loss:0.24336913228034973\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}