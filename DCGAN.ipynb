{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0H8VSHA0VyDnrjbSxwzfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kallviktor/RandomInterpolationGAN/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C12B_Jtthu2r"
      },
      "source": [
        "https://github.com/kmualim/DCGAN-Keras-Implementation/blob/master/dcgan-mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phK8FR8Vfwlq",
        "outputId": "3bf34eda-95b6-4811-a950-94f6af5d659a"
      },
      "source": [
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Reshape \r\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D \r\n",
        "from keras.layers import LeakyReLU, Dropout \r\n",
        "from keras.layers import BatchNormalization \r\n",
        "from keras.optimizers import Adam, RMSprop\r\n",
        "from keras.initializers import RandomNormal, Zeros\r\n",
        "from keras.datasets import mnist\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import sys \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class GAN(object): \r\n",
        "    def __init__(self):\r\n",
        "      self.img_rows = 28 \r\n",
        "      self.img_cols = 28 \r\n",
        "      self.channel=1\r\n",
        "      self.img_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "      \r\n",
        "      optimizer = Adam(0.0002, 0.5)\r\n",
        "     \r\n",
        "    def build_discriminator(self):\r\n",
        "      model = Sequential()\r\n",
        "      depth = 32 \r\n",
        "      dropout=0.25 \r\n",
        "      input_shape = (self.img_rows, self.img_cols, self.channel)\r\n",
        "      \r\n",
        "      model.add(Conv2D(depth*1, 3, strides=2, input_shape=input_shape, padding='same', kernel_initializer='random_uniform'))\r\n",
        "      model.add(BatchNormalization(momentum=0.9))\r\n",
        "      model.add(LeakyReLU(alpha=0.2))\r\n",
        "      model.add(Dropout(dropout))\r\n",
        "      model.add(Conv2D(depth*2, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      model.add(BatchNormalization(momentum=0.9))\r\n",
        "      model.add(LeakyReLU(alpha=0.2))\r\n",
        "      model.add(Dropout(dropout))\r\n",
        "      model.add(Conv2D(depth*4, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      model.add(BatchNormalization(momentum=0.9))\r\n",
        "      model.add(LeakyReLU(alpha=0.2))\r\n",
        "      model.add(Dropout(dropout))\r\n",
        "      model.add(Conv2D(depth*8, 3, strides=2, padding='same',kernel_initializer='random_uniform'))\r\n",
        "      model.add(BatchNormalization(momentum=0.9))\r\n",
        "      model.add(LeakyReLU(alpha=0.2))\r\n",
        "      model.add(Dropout(dropout))\r\n",
        "\r\n",
        "      # Each MNIST input = 28 X 28 X 1, depth = 1\r\n",
        "      # Each Output = 14 X 14 X 1, depth = 64 \r\n",
        "      # Model has 4 convolutional layer, each with a dropout layer in between \r\n",
        "\r\n",
        "      # Output \r\n",
        "      model.add(Flatten())\r\n",
        "      model.add(Dense(1))\r\n",
        "      model.add(Activation('sigmoid'))\r\n",
        "      model.summary()\r\n",
        "      \r\n",
        "      img = Input(shape=(self.img_shape))\r\n",
        "      validity = model(img)\r\n",
        "      \r\n",
        "      return Model(img, validity) \r\n",
        "\r\n",
        "    # generator takes noise as input and generates imgs\r\n",
        "                \r\n",
        "    def build_generator(self):\r\n",
        "      generator = Sequential() \r\n",
        "      dropout = 0.4 \r\n",
        "      depth = 128\r\n",
        "      dim = 7\r\n",
        "\r\n",
        "      # In: 100 \r\n",
        "      # Out: dim X dim X depth \r\n",
        "\r\n",
        "      generator.add(Dense(dim*dim*depth, input_dim=100))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "      generator.add(Reshape((dim, dim, depth)))\r\n",
        "      generator.add(UpSampling2D())\r\n",
        "      #generator.add(Dropout(dropout))\r\n",
        "\r\n",
        "      # In: dim X dim X depth\r\n",
        "      # Out: 2*dim X 2*dim X depth/2 \r\n",
        "\r\n",
        "      generator.add(Conv2D(depth, 3, padding='same'))\r\n",
        "      generator.add(BatchNormalization(momentum=0.9))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "      generator.add(UpSampling2D())\r\n",
        "      generator.add(Conv2D(int(depth/2), 3, padding='same'))\r\n",
        "      generator.add(BatchNormalization(momentum=0.9))\r\n",
        "      generator.add(Activation('relu'))\r\n",
        "     \r\n",
        "\r\n",
        "      # Out : 28 X 28 X 1 grayscale image [0.0, 1.0] per pix\r\n",
        "      generator.add(Conv2D(1,3,padding='same'))\r\n",
        "      generator.add(Activation('tanh'))\r\n",
        "      generator.summary()\r\n",
        "      \r\n",
        "      noise = Input(shape=(100,))\r\n",
        "      img = generator(noise)\r\n",
        "      \r\n",
        "      return Model(noise, img)\r\n",
        "    \r\n",
        "    # Build and compile discriminator\r\n",
        "    def DM(self):\r\n",
        "      optimizer = Adam(0.0002, 0.5)\r\n",
        "      DM = self.build_discriminator()\r\n",
        "      DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n",
        "      return DM    \r\n",
        "\r\n",
        "class dcgan(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.img_rows=28\r\n",
        "    self.img_cols=28\r\n",
        "    self.channels=1\r\n",
        "\r\n",
        "    # building the generator \r\n",
        "    self.GAN = GAN()\r\n",
        "    self.DM = self.GAN.DM()\r\n",
        "    self.generator = self.GAN.build_generator()\r\n",
        "\r\n",
        "\r\n",
        "    z = Input(shape=(100,))\r\n",
        "    img = self.generator(z)\r\n",
        "    self.DM.trainable = False\r\n",
        "    valid = self.DM(img)\r\n",
        "    \r\n",
        "    self.combined = Model(z, valid)\r\n",
        "    optimizer = Adam(0.0002, 0.5)\r\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n",
        "    \r\n",
        "    # training input \r\n",
        "    # To change dataset, place dataset below \r\n",
        "    (self.x_train, _), (_,_) = mnist.load_data()\r\n",
        "    self.x_train = self.x_train/127.5 -1.\r\n",
        "    self.x_train = np.expand_dims(self.x_train, axis=3) \r\n",
        "    #x_train = x_train/127.5 -1. \r\n",
        "    #x_train = np.expand_dims(x_train, axis=3)\r\n",
        "    self.n_samples = 25\r\n",
        "    self.noise_dim = 100\r\n",
        "  \r\n",
        "  # method to generate noise \r\n",
        "  def gennoise(self,batch_size, noise_dim): \r\n",
        "   \tx = np.random.normal(0, 1.0, (batch_size, self.noise_dim))\r\n",
        "   \treturn x\r\n",
        "\r\n",
        "  def plt_imgs(self,epoch): \r\n",
        "    noise = self.gennoise(self.n_samples, self.noise_dim)\r\n",
        "    fake_imgs = self.generator.predict(noise)\r\n",
        "    fake_imgs = 0.5 * fake_imgs + 0.5\r\n",
        "  \r\n",
        "    fig,axs = plt.subplots(5,5)\r\n",
        "    count = 0 \r\n",
        "    for i in range(5): \r\n",
        "        for j in range(5): \r\n",
        "          axs[i,j].imshow(fake_imgs[count, :, :, 0], cmap='gray')\r\n",
        "          axs[i,j].axis('off')\r\n",
        "          count+=1\r\n",
        "      \r\n",
        "    fig.savefig(\"mnist_%d.png\" % epoch)\r\n",
        "    plt.close()\r\n",
        "  \r\n",
        "  \r\n",
        "  def train(self,n_epochs, batch_size):\r\n",
        "    train_hist={}\r\n",
        "    train_hist['D_losses']=[]\r\n",
        "    train_hist['G_losses']=[]\r\n",
        "    print(\"Start\")\r\n",
        "    true_labels=np.ones((batch_size,1))\r\n",
        "    gen_gene_labels=np.zeros((batch_size,1))\r\n",
        "    \r\n",
        "    for epoch in range(n_epochs):\r\n",
        "    \r\n",
        "      index = np.random.randint(0, self.x_train.shape[0], batch_size)\r\n",
        "      images = self.x_train[index]\r\n",
        "      \r\n",
        "      noise_data = self.gennoise(batch_size, 100)\r\n",
        "      gen_imgs = self.generator.predict(noise_data)\r\n",
        "      \r\n",
        "      \r\n",
        "      d_loss = self.DM.train_on_batch(images, true_labels)\r\n",
        "    \r\n",
        "      d_loss_generated = self.DM.train_on_batch(gen_imgs, gen_gene_labels)\r\n",
        "      \r\n",
        "      total_d_loss = 0.5 * np.add(d_loss, d_loss_generated)\r\n",
        "      \r\n",
        "      train_hist['D_losses'].append(total_d_loss[0])\r\n",
        "        \r\n",
        "      noise_data = self.gennoise(batch_size, 100)\r\n",
        "      y1 = np.ones((batch_size, 1))    \r\n",
        "      \r\n",
        "      g_loss = self.combined.train_on_batch(noise_data, y1)\r\n",
        "\r\n",
        "      train_hist['G_losses'].append(g_loss)\r\n",
        "      print (' Epoch:{}, G_loss: {}, D_loss:{}'.format(epoch+1, g_loss, total_d_loss[0]))\r\n",
        "      \r\n",
        "      if epoch%50==0:\r\n",
        "        self.plt_imgs(epoch)\r\n",
        "      \r\n",
        "    return train_hist\r\n",
        "  \r\n",
        "  def plotting_imgs(self,epoch): \r\n",
        "      noise = self.gennoise(25,100)\r\n",
        "      fake_imgs = self.generator.predict(noise)\r\n",
        "      fake_imgs = 0.5 * fake_imgs + 0.5\r\n",
        "    \r\n",
        "      fig,axs = plt.subplots(5,5)\r\n",
        "      count = 0 \r\n",
        "      for i in range(5): \r\n",
        "        for j in range(5): \r\n",
        "          axs[i,j].imshow(fake_imgs[count, :, :, 0], cmap='gray')\r\n",
        "          axs[i,j].axis('off')\r\n",
        "          count+=1\r\n",
        "\r\n",
        "if __name__ == '__main__': \r\n",
        "  mnist_dcgan = dcgan()\r\n",
        "  train_hist = mnist_dcgan.train(400, batch_size=32)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1025      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 390,785\n",
            "Trainable params: 389,825\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Start\n",
            " Epoch:1, G_loss: 0.6610168218612671, D_loss:0.9670487344264984\n",
            " Epoch:2, G_loss: 0.6358987092971802, D_loss:0.6849463731050491\n",
            " Epoch:3, G_loss: 0.6373149752616882, D_loss:0.5982841700315475\n",
            " Epoch:4, G_loss: 0.6258140802383423, D_loss:0.5058718174695969\n",
            " Epoch:5, G_loss: 0.6253659725189209, D_loss:0.3778887242078781\n",
            " Epoch:6, G_loss: 0.6195757389068604, D_loss:0.2564794719219208\n",
            " Epoch:7, G_loss: 0.6502702236175537, D_loss:0.21718142926692963\n",
            " Epoch:8, G_loss: 0.6154075860977173, D_loss:0.29120003432035446\n",
            " Epoch:9, G_loss: 0.6276384592056274, D_loss:0.1662343442440033\n",
            " Epoch:10, G_loss: 0.5822373628616333, D_loss:0.21297527104616165\n",
            " Epoch:11, G_loss: 0.5797095894813538, D_loss:0.15709885582327843\n",
            " Epoch:12, G_loss: 0.5734164118766785, D_loss:0.1264243647456169\n",
            " Epoch:13, G_loss: 0.5957475304603577, D_loss:0.14961231127381325\n",
            " Epoch:14, G_loss: 0.5553040504455566, D_loss:0.11630143970251083\n",
            " Epoch:15, G_loss: 0.492636501789093, D_loss:0.13170376420021057\n",
            " Epoch:16, G_loss: 0.44909244775772095, D_loss:0.1392078697681427\n",
            " Epoch:17, G_loss: 0.4489046633243561, D_loss:0.11925352364778519\n",
            " Epoch:18, G_loss: 0.5335464477539062, D_loss:0.07170851528644562\n",
            " Epoch:19, G_loss: 0.3974081873893738, D_loss:0.12090552225708961\n",
            " Epoch:20, G_loss: 0.5565048456192017, D_loss:0.1325291432440281\n",
            " Epoch:21, G_loss: 0.4832366108894348, D_loss:0.18462160974740982\n",
            " Epoch:22, G_loss: 0.6699478626251221, D_loss:0.16881636157631874\n",
            " Epoch:23, G_loss: 0.8205248713493347, D_loss:0.23146680742502213\n",
            " Epoch:24, G_loss: 0.8264089226722717, D_loss:0.21820220351219177\n",
            " Epoch:25, G_loss: 0.8611595630645752, D_loss:0.30641746520996094\n",
            " Epoch:26, G_loss: 0.6999319791793823, D_loss:0.332690104842186\n",
            " Epoch:27, G_loss: 0.7653508186340332, D_loss:0.23675285279750824\n",
            " Epoch:28, G_loss: 0.9906709790229797, D_loss:0.36162345111370087\n",
            " Epoch:29, G_loss: 1.3945016860961914, D_loss:0.2639024704694748\n",
            " Epoch:30, G_loss: 2.007042407989502, D_loss:0.5651015043258667\n",
            " Epoch:31, G_loss: 1.9854649305343628, D_loss:0.47165700793266296\n",
            " Epoch:32, G_loss: 1.7476716041564941, D_loss:0.34205159544944763\n",
            " Epoch:33, G_loss: 1.6917800903320312, D_loss:0.40234656631946564\n",
            " Epoch:34, G_loss: 1.4622271060943604, D_loss:0.38731399178504944\n",
            " Epoch:35, G_loss: 1.652292251586914, D_loss:0.24900445342063904\n",
            " Epoch:36, G_loss: 1.1974914073944092, D_loss:0.2177756428718567\n",
            " Epoch:37, G_loss: 0.9781363010406494, D_loss:0.15835324674844742\n",
            " Epoch:38, G_loss: 0.9027104377746582, D_loss:0.21604546904563904\n",
            " Epoch:39, G_loss: 1.0947452783584595, D_loss:0.1091226264834404\n",
            " Epoch:40, G_loss: 0.9358781576156616, D_loss:0.12002227082848549\n",
            " Epoch:41, G_loss: 1.2197015285491943, D_loss:0.10340263694524765\n",
            " Epoch:42, G_loss: 1.099663257598877, D_loss:0.06936699524521828\n",
            " Epoch:43, G_loss: 1.1417948007583618, D_loss:0.08857422694563866\n",
            " Epoch:44, G_loss: 1.4536960124969482, D_loss:0.12024137750267982\n",
            " Epoch:45, G_loss: 1.9363222122192383, D_loss:0.15576165169477463\n",
            " Epoch:46, G_loss: 2.290626287460327, D_loss:0.25308696180582047\n",
            " Epoch:47, G_loss: 2.1060919761657715, D_loss:0.4756269156932831\n",
            " Epoch:48, G_loss: 2.6103615760803223, D_loss:0.5805637240409851\n",
            " Epoch:49, G_loss: 2.4709808826446533, D_loss:0.5014112889766693\n",
            " Epoch:50, G_loss: 1.9599229097366333, D_loss:0.5277242362499237\n",
            " Epoch:51, G_loss: 1.5846154689788818, D_loss:0.46374112367630005\n",
            " Epoch:52, G_loss: 1.7557505369186401, D_loss:0.35647329688072205\n",
            " Epoch:53, G_loss: 2.0125885009765625, D_loss:0.36281339824199677\n",
            " Epoch:54, G_loss: 1.3296594619750977, D_loss:0.24390501528978348\n",
            " Epoch:55, G_loss: 1.468971610069275, D_loss:0.21871241182088852\n",
            " Epoch:56, G_loss: 1.2108755111694336, D_loss:0.16157237440347672\n",
            " Epoch:57, G_loss: 1.539865493774414, D_loss:0.13219378888607025\n",
            " Epoch:58, G_loss: 0.7362889647483826, D_loss:0.13171802088618279\n",
            " Epoch:59, G_loss: 0.9503053426742554, D_loss:0.1483212485909462\n",
            " Epoch:60, G_loss: 0.8791060447692871, D_loss:0.04523412510752678\n",
            " Epoch:61, G_loss: 0.7484952807426453, D_loss:0.09881564974784851\n",
            " Epoch:62, G_loss: 0.9459565281867981, D_loss:0.037886764854192734\n",
            " Epoch:63, G_loss: 0.5658842921257019, D_loss:0.08393394015729427\n",
            " Epoch:64, G_loss: 0.6047762632369995, D_loss:0.05236105062067509\n",
            " Epoch:65, G_loss: 0.5517383217811584, D_loss:0.028116301633417606\n",
            " Epoch:66, G_loss: 0.5973769426345825, D_loss:0.03692498803138733\n",
            " Epoch:67, G_loss: 0.35610830783843994, D_loss:0.08197096362709999\n",
            " Epoch:68, G_loss: 0.3741089701652527, D_loss:0.024239310063421726\n",
            " Epoch:69, G_loss: 0.6068885922431946, D_loss:0.030870532616972923\n",
            " Epoch:70, G_loss: 0.49068117141723633, D_loss:0.04314378835260868\n",
            " Epoch:71, G_loss: 0.949517011642456, D_loss:0.061651455238461494\n",
            " Epoch:72, G_loss: 1.0022521018981934, D_loss:0.04224756918847561\n",
            " Epoch:73, G_loss: 0.8036161661148071, D_loss:0.020924162119627\n",
            " Epoch:74, G_loss: 0.9118066430091858, D_loss:0.027313878759741783\n",
            " Epoch:75, G_loss: 0.7182461023330688, D_loss:0.04594320058822632\n",
            " Epoch:76, G_loss: 0.8920687437057495, D_loss:0.04668842442333698\n",
            " Epoch:77, G_loss: 0.8028005361557007, D_loss:0.04780040867626667\n",
            " Epoch:78, G_loss: 1.2663743495941162, D_loss:0.04708651639521122\n",
            " Epoch:79, G_loss: 1.112706184387207, D_loss:0.062391060404479504\n",
            " Epoch:80, G_loss: 1.1284356117248535, D_loss:0.11815128475427628\n",
            " Epoch:81, G_loss: 1.2654081583023071, D_loss:0.09936001524329185\n",
            " Epoch:82, G_loss: 1.298386573791504, D_loss:0.17240479588508606\n",
            " Epoch:83, G_loss: 1.041614055633545, D_loss:0.32049837708473206\n",
            " Epoch:84, G_loss: 1.4408087730407715, D_loss:0.18746639788150787\n",
            " Epoch:85, G_loss: 1.1772972345352173, D_loss:0.25868910551071167\n",
            " Epoch:86, G_loss: 1.0572559833526611, D_loss:0.2760477811098099\n",
            " Epoch:87, G_loss: 1.4094023704528809, D_loss:0.056154659017920494\n",
            " Epoch:88, G_loss: 0.9306389093399048, D_loss:0.17434494197368622\n",
            " Epoch:89, G_loss: 1.0070246458053589, D_loss:0.0694990772753954\n",
            " Epoch:90, G_loss: 0.984468936920166, D_loss:0.02851019147783518\n",
            " Epoch:91, G_loss: 0.7536341547966003, D_loss:0.05786381661891937\n",
            " Epoch:92, G_loss: 0.7407695055007935, D_loss:0.024097478948533535\n",
            " Epoch:93, G_loss: 0.6030059456825256, D_loss:0.02063765050843358\n",
            " Epoch:94, G_loss: 0.4010210633277893, D_loss:0.032130300998687744\n",
            " Epoch:95, G_loss: 0.5652469992637634, D_loss:0.021501274779438972\n",
            " Epoch:96, G_loss: 0.5974408388137817, D_loss:0.019418859854340553\n",
            " Epoch:97, G_loss: 0.6647862195968628, D_loss:0.02091142488643527\n",
            " Epoch:98, G_loss: 0.8643062114715576, D_loss:0.014815843664109707\n",
            " Epoch:99, G_loss: 0.4096905589103699, D_loss:0.03669980261474848\n",
            " Epoch:100, G_loss: 0.6280784010887146, D_loss:0.016269615851342678\n",
            " Epoch:101, G_loss: 0.8667284250259399, D_loss:0.024445248767733574\n",
            " Epoch:102, G_loss: 0.8307055234909058, D_loss:0.01818963885307312\n",
            " Epoch:103, G_loss: 0.8976684808731079, D_loss:0.0254043685272336\n",
            " Epoch:104, G_loss: 0.8121800422668457, D_loss:0.044202400371432304\n",
            " Epoch:105, G_loss: 1.1130223274230957, D_loss:0.07038154732435942\n",
            " Epoch:106, G_loss: 0.8713146448135376, D_loss:0.10743076354265213\n",
            " Epoch:107, G_loss: 1.255070686340332, D_loss:0.12298348173499107\n",
            " Epoch:108, G_loss: 1.5096850395202637, D_loss:0.045786041766405106\n",
            " Epoch:109, G_loss: 0.828614354133606, D_loss:0.2545379400253296\n",
            " Epoch:110, G_loss: 1.3046815395355225, D_loss:0.12973893247544765\n",
            " Epoch:111, G_loss: 1.089586615562439, D_loss:0.3195507079362869\n",
            " Epoch:112, G_loss: 1.3695755004882812, D_loss:0.05533640831708908\n",
            " Epoch:113, G_loss: 0.9709516763687134, D_loss:0.30561453104019165\n",
            " Epoch:114, G_loss: 1.102616310119629, D_loss:0.06599156558513641\n",
            " Epoch:115, G_loss: 0.67586350440979, D_loss:0.12005116790533066\n",
            " Epoch:116, G_loss: 0.6374205350875854, D_loss:0.045210002921521664\n",
            " Epoch:117, G_loss: 0.6833062767982483, D_loss:0.031500108540058136\n",
            " Epoch:118, G_loss: 0.6633154153823853, D_loss:0.017290241084992886\n",
            " Epoch:119, G_loss: 0.7900814414024353, D_loss:0.017992722801864147\n",
            " Epoch:120, G_loss: 0.6084203720092773, D_loss:0.07228317484259605\n",
            " Epoch:121, G_loss: 0.7266411781311035, D_loss:0.03592194803059101\n",
            " Epoch:122, G_loss: 0.8402336239814758, D_loss:0.03100164420902729\n",
            " Epoch:123, G_loss: 0.7460880875587463, D_loss:0.07970480248332024\n",
            " Epoch:124, G_loss: 0.892627477645874, D_loss:0.03184891864657402\n",
            " Epoch:125, G_loss: 0.5676796436309814, D_loss:0.03603217611089349\n",
            " Epoch:126, G_loss: 0.36863285303115845, D_loss:0.015526722185313702\n",
            " Epoch:127, G_loss: 0.7637888193130493, D_loss:0.039989820681512356\n",
            " Epoch:128, G_loss: 0.8894284963607788, D_loss:0.011292254785075784\n",
            " Epoch:129, G_loss: 0.9775760173797607, D_loss:0.04954375512897968\n",
            " Epoch:130, G_loss: 0.9710485935211182, D_loss:0.0772932581603527\n",
            " Epoch:131, G_loss: 0.8710616827011108, D_loss:0.026356997434049845\n",
            " Epoch:132, G_loss: 1.0230038166046143, D_loss:0.03289405815303326\n",
            " Epoch:133, G_loss: 0.7801461219787598, D_loss:0.04293726943433285\n",
            " Epoch:134, G_loss: 1.1074155569076538, D_loss:0.08005449920892715\n",
            " Epoch:135, G_loss: 1.0079189538955688, D_loss:0.03714369051158428\n",
            " Epoch:136, G_loss: 0.5291966795921326, D_loss:0.08457611501216888\n",
            " Epoch:137, G_loss: 0.494188129901886, D_loss:0.07884231582283974\n",
            " Epoch:138, G_loss: 0.782149076461792, D_loss:0.020537077449262142\n",
            " Epoch:139, G_loss: 0.8690404891967773, D_loss:0.024189706426113844\n",
            " Epoch:140, G_loss: 0.7645114660263062, D_loss:0.033605684991925955\n",
            " Epoch:141, G_loss: 0.6949958205223083, D_loss:0.06672517582774162\n",
            " Epoch:142, G_loss: 0.502780556678772, D_loss:0.018949417863041162\n",
            " Epoch:143, G_loss: 0.8818668127059937, D_loss:0.036921300925314426\n",
            " Epoch:144, G_loss: 1.422982096672058, D_loss:0.16006259620189667\n",
            " Epoch:145, G_loss: 1.5645530223846436, D_loss:0.05423459969460964\n",
            " Epoch:146, G_loss: 1.076688289642334, D_loss:0.052694909274578094\n",
            " Epoch:147, G_loss: 1.215876579284668, D_loss:0.06169270537793636\n",
            " Epoch:148, G_loss: 1.1505496501922607, D_loss:0.09823088347911835\n",
            " Epoch:149, G_loss: 1.3380378484725952, D_loss:0.11476999893784523\n",
            " Epoch:150, G_loss: 2.0128278732299805, D_loss:0.2830585539340973\n",
            " Epoch:151, G_loss: 1.9876514673233032, D_loss:1.4191815555095673\n",
            " Epoch:152, G_loss: 2.192835807800293, D_loss:0.1664551291614771\n",
            " Epoch:153, G_loss: 0.8650251030921936, D_loss:0.08961650170385838\n",
            " Epoch:154, G_loss: 0.6792429089546204, D_loss:0.02254853956401348\n",
            " Epoch:155, G_loss: 0.6266453266143799, D_loss:0.017735144589096308\n",
            " Epoch:156, G_loss: 0.3656200170516968, D_loss:0.04365053935907781\n",
            " Epoch:157, G_loss: 0.32738107442855835, D_loss:0.01630309224128723\n",
            " Epoch:158, G_loss: 0.42686063051223755, D_loss:0.013510553166270256\n",
            " Epoch:159, G_loss: 0.3769151568412781, D_loss:0.0052113002166152\n",
            " Epoch:160, G_loss: 0.3623468279838562, D_loss:0.007165431976318359\n",
            " Epoch:161, G_loss: 0.39754945039749146, D_loss:0.0061091461684554815\n",
            " Epoch:162, G_loss: 0.5117421746253967, D_loss:0.004571329802274704\n",
            " Epoch:163, G_loss: 0.44969940185546875, D_loss:0.010332925245165825\n",
            " Epoch:164, G_loss: 0.7422810792922974, D_loss:0.00576112320413813\n",
            " Epoch:165, G_loss: 0.37752819061279297, D_loss:0.002102731610648334\n",
            " Epoch:166, G_loss: 0.7609274387359619, D_loss:0.003745859023183584\n",
            " Epoch:167, G_loss: 0.6519333124160767, D_loss:0.007139574270695448\n",
            " Epoch:168, G_loss: 0.6515085697174072, D_loss:0.005033469060435891\n",
            " Epoch:169, G_loss: 0.6121318936347961, D_loss:0.0032283132895827293\n",
            " Epoch:170, G_loss: 0.6698733568191528, D_loss:0.008308709599077702\n",
            " Epoch:171, G_loss: 0.614890456199646, D_loss:0.010743095073848963\n",
            " Epoch:172, G_loss: 0.5979689955711365, D_loss:0.007314554415643215\n",
            " Epoch:173, G_loss: 0.5020734071731567, D_loss:0.015714706853032112\n",
            " Epoch:174, G_loss: 0.765561580657959, D_loss:0.030979592353105545\n",
            " Epoch:175, G_loss: 0.8611507415771484, D_loss:0.007723997114226222\n",
            " Epoch:176, G_loss: 0.7673689723014832, D_loss:0.010810188949108124\n",
            " Epoch:177, G_loss: 0.7085707187652588, D_loss:0.0050613428466022015\n",
            " Epoch:178, G_loss: 0.6058716177940369, D_loss:0.02791561372578144\n",
            " Epoch:179, G_loss: 0.7349960803985596, D_loss:0.01087354775518179\n",
            " Epoch:180, G_loss: 0.891139566898346, D_loss:0.015429860912263393\n",
            " Epoch:181, G_loss: 0.7364510297775269, D_loss:0.01893617818132043\n",
            " Epoch:182, G_loss: 0.9077447652816772, D_loss:0.013394580222666264\n",
            " Epoch:183, G_loss: 0.6430923938751221, D_loss:0.01817137748003006\n",
            " Epoch:184, G_loss: 0.8027101755142212, D_loss:0.011492269113659859\n",
            " Epoch:185, G_loss: 1.124658465385437, D_loss:0.0774556277319789\n",
            " Epoch:186, G_loss: 1.6362080574035645, D_loss:0.0401095999404788\n",
            " Epoch:187, G_loss: 0.8334610462188721, D_loss:0.02950315037742257\n",
            " Epoch:188, G_loss: 0.8089858293533325, D_loss:0.009474982973188162\n",
            " Epoch:189, G_loss: 0.8860388994216919, D_loss:0.0844589676707983\n",
            " Epoch:190, G_loss: 1.1907669305801392, D_loss:0.026195233687758446\n",
            " Epoch:191, G_loss: 1.0154670476913452, D_loss:0.24457859992980957\n",
            " Epoch:192, G_loss: 0.9741894602775574, D_loss:0.07989994995296001\n",
            " Epoch:193, G_loss: 1.1252881288528442, D_loss:0.075209841132164\n",
            " Epoch:194, G_loss: 1.095629096031189, D_loss:0.05622212216258049\n",
            " Epoch:195, G_loss: 1.1443742513656616, D_loss:0.07514060288667679\n",
            " Epoch:196, G_loss: 2.637819766998291, D_loss:0.8695362210273743\n",
            " Epoch:197, G_loss: 1.398268461227417, D_loss:0.5671225935220718\n",
            " Epoch:198, G_loss: 1.266882061958313, D_loss:0.054120179265737534\n",
            " Epoch:199, G_loss: 1.5497686862945557, D_loss:0.020377459935843945\n",
            " Epoch:200, G_loss: 0.8210367560386658, D_loss:0.12089318782091141\n",
            " Epoch:201, G_loss: 0.9693058133125305, D_loss:0.14730583876371384\n",
            " Epoch:202, G_loss: 1.182520866394043, D_loss:0.030586836859583855\n",
            " Epoch:203, G_loss: 0.9027953743934631, D_loss:0.3199225813150406\n",
            " Epoch:204, G_loss: 0.7285075783729553, D_loss:0.04966648295521736\n",
            " Epoch:205, G_loss: 0.5094957947731018, D_loss:0.06588126346468925\n",
            " Epoch:206, G_loss: 0.5285826921463013, D_loss:0.04066177271306515\n",
            " Epoch:207, G_loss: 0.5301187038421631, D_loss:0.013958094641566277\n",
            " Epoch:208, G_loss: 0.608139157295227, D_loss:0.012485023587942123\n",
            " Epoch:209, G_loss: 0.8385453224182129, D_loss:0.01898507960140705\n",
            " Epoch:210, G_loss: 0.6438854336738586, D_loss:0.051002184860408306\n",
            " Epoch:211, G_loss: 0.5176677703857422, D_loss:0.008568738237954676\n",
            " Epoch:212, G_loss: 0.5674468278884888, D_loss:0.05258052237331867\n",
            " Epoch:213, G_loss: 0.43219077587127686, D_loss:0.04103975370526314\n",
            " Epoch:214, G_loss: 0.39496374130249023, D_loss:0.018048429396003485\n",
            " Epoch:215, G_loss: 1.3963918685913086, D_loss:0.15797802433371544\n",
            " Epoch:216, G_loss: 0.868614673614502, D_loss:0.21906976401805878\n",
            " Epoch:217, G_loss: 0.6945074796676636, D_loss:0.0748191811144352\n",
            " Epoch:218, G_loss: 0.744659960269928, D_loss:0.04249355546198785\n",
            " Epoch:219, G_loss: 1.2210242748260498, D_loss:0.1295803114771843\n",
            " Epoch:220, G_loss: 1.386157512664795, D_loss:0.3036912977695465\n",
            " Epoch:221, G_loss: 1.5664992332458496, D_loss:0.07802801951766014\n",
            " Epoch:222, G_loss: 1.3812626600265503, D_loss:1.1583042740821838\n",
            " Epoch:223, G_loss: 1.7932136058807373, D_loss:0.1634622933343053\n",
            " Epoch:224, G_loss: 0.8847895860671997, D_loss:0.12636019103229046\n",
            " Epoch:225, G_loss: 0.34559494256973267, D_loss:0.021831730380654335\n",
            " Epoch:226, G_loss: 0.5087544918060303, D_loss:0.004316143691539764\n",
            " Epoch:227, G_loss: 0.5800195336341858, D_loss:0.0032940316013991833\n",
            " Epoch:228, G_loss: 0.4827630817890167, D_loss:0.003482706262730062\n",
            " Epoch:229, G_loss: 0.4321969151496887, D_loss:0.0036702663055621088\n",
            " Epoch:230, G_loss: 0.6336218118667603, D_loss:0.017106226063333452\n",
            " Epoch:231, G_loss: 0.5008379817008972, D_loss:0.003611889900639653\n",
            " Epoch:232, G_loss: 0.5750137567520142, D_loss:0.004914290737360716\n",
            " Epoch:233, G_loss: 0.5914410948753357, D_loss:0.0025362824089825153\n",
            " Epoch:234, G_loss: 0.482746422290802, D_loss:0.0034726845333352685\n",
            " Epoch:235, G_loss: 0.758408784866333, D_loss:0.0017934607458300889\n",
            " Epoch:236, G_loss: 0.581592321395874, D_loss:0.0020808768458664417\n",
            " Epoch:237, G_loss: 0.7237817049026489, D_loss:0.001964689639862627\n",
            " Epoch:238, G_loss: 0.5965070128440857, D_loss:0.0028007114597130567\n",
            " Epoch:239, G_loss: 0.8875638246536255, D_loss:0.006783098913729191\n",
            " Epoch:240, G_loss: 0.6455708742141724, D_loss:0.0027955099067185074\n",
            " Epoch:241, G_loss: 0.7595721483230591, D_loss:0.0021381122060120106\n",
            " Epoch:242, G_loss: 0.6932089328765869, D_loss:0.002103859034832567\n",
            " Epoch:243, G_loss: 0.4858960211277008, D_loss:0.004391196882352233\n",
            " Epoch:244, G_loss: 0.5937484502792358, D_loss:0.001772095449268818\n",
            " Epoch:245, G_loss: 0.5443075299263, D_loss:0.0023394764866679907\n",
            " Epoch:246, G_loss: 0.6356081366539001, D_loss:0.0008347414550371468\n",
            " Epoch:247, G_loss: 0.5511509776115417, D_loss:0.001271166023798287\n",
            " Epoch:248, G_loss: 0.5042260885238647, D_loss:0.001220031874254346\n",
            " Epoch:249, G_loss: 0.5139192342758179, D_loss:0.001425278082024306\n",
            " Epoch:250, G_loss: 0.36142227053642273, D_loss:0.0015962850884534419\n",
            " Epoch:251, G_loss: 0.7402291893959045, D_loss:0.0011752488790079951\n",
            " Epoch:252, G_loss: 0.3985077738761902, D_loss:0.0012498704018071294\n",
            " Epoch:253, G_loss: 0.7698168754577637, D_loss:0.002213828032836318\n",
            " Epoch:254, G_loss: 0.6265956163406372, D_loss:0.001352379098534584\n",
            " Epoch:255, G_loss: 0.7267298698425293, D_loss:0.003164698136970401\n",
            " Epoch:256, G_loss: 0.5292184352874756, D_loss:0.001647261728066951\n",
            " Epoch:257, G_loss: 0.5214969515800476, D_loss:0.003136100771371275\n",
            " Epoch:258, G_loss: 0.5977667570114136, D_loss:0.0010281111462973058\n",
            " Epoch:259, G_loss: 0.530753493309021, D_loss:0.0031513447174802423\n",
            " Epoch:260, G_loss: 0.6243581771850586, D_loss:0.0007892203866504133\n",
            " Epoch:261, G_loss: 0.7948477268218994, D_loss:0.002132726483978331\n",
            " Epoch:262, G_loss: 0.5186518430709839, D_loss:0.00184284255374223\n",
            " Epoch:263, G_loss: 0.6377019882202148, D_loss:0.0017997430986724794\n",
            " Epoch:264, G_loss: 0.8021683692932129, D_loss:0.0011911182082258165\n",
            " Epoch:265, G_loss: 0.4104136526584625, D_loss:0.00222295755520463\n",
            " Epoch:266, G_loss: 0.453042596578598, D_loss:0.0010792030952870846\n",
            " Epoch:267, G_loss: 0.6236413717269897, D_loss:0.0008877859800122678\n",
            " Epoch:268, G_loss: 0.45672154426574707, D_loss:0.0012586800148710608\n",
            " Epoch:269, G_loss: 0.49957844614982605, D_loss:0.0006826756289228797\n",
            " Epoch:270, G_loss: 0.46677765250205994, D_loss:0.0016726278699934483\n",
            " Epoch:271, G_loss: 0.49119776487350464, D_loss:0.0006894658727105707\n",
            " Epoch:272, G_loss: 0.5419535040855408, D_loss:0.0018202849896624684\n",
            " Epoch:273, G_loss: 0.6059226989746094, D_loss:0.004408365581184626\n",
            " Epoch:274, G_loss: 0.547217845916748, D_loss:0.001116239174734801\n",
            " Epoch:275, G_loss: 0.4720948338508606, D_loss:0.0032931995228864253\n",
            " Epoch:276, G_loss: 0.4608306884765625, D_loss:0.0014583373558707535\n",
            " Epoch:277, G_loss: 0.4837234318256378, D_loss:0.0010802073520608246\n",
            " Epoch:278, G_loss: 0.45478832721710205, D_loss:0.006196398055180907\n",
            " Epoch:279, G_loss: 0.5440843105316162, D_loss:0.0021563131595030427\n",
            " Epoch:280, G_loss: 0.6163607835769653, D_loss:0.0016563392709940672\n",
            " Epoch:281, G_loss: 0.4278331398963928, D_loss:0.0007606707804370672\n",
            " Epoch:282, G_loss: 0.5933003425598145, D_loss:0.0022432742407545447\n",
            " Epoch:283, G_loss: 0.713482677936554, D_loss:0.0009749869932420552\n",
            " Epoch:284, G_loss: 0.5493918061256409, D_loss:0.000817736960016191\n",
            " Epoch:285, G_loss: 0.45956456661224365, D_loss:0.0007681833230890334\n",
            " Epoch:286, G_loss: 0.36010730266571045, D_loss:0.001608924358151853\n",
            " Epoch:287, G_loss: 0.5151076316833496, D_loss:0.00637945716152899\n",
            " Epoch:288, G_loss: 0.5925917029380798, D_loss:0.0004363539774203673\n",
            " Epoch:289, G_loss: 0.6235262155532837, D_loss:0.0014334150619106367\n",
            " Epoch:290, G_loss: 0.7230762243270874, D_loss:0.0007677191752009094\n",
            " Epoch:291, G_loss: 0.6989175081253052, D_loss:0.0012675325851887465\n",
            " Epoch:292, G_loss: 0.7115800380706787, D_loss:0.0017832026933319867\n",
            " Epoch:293, G_loss: 0.6128613948822021, D_loss:0.009109480190090835\n",
            " Epoch:294, G_loss: 0.5076102614402771, D_loss:0.0017648651846684515\n",
            " Epoch:295, G_loss: 0.5636967420578003, D_loss:0.0009537447476759553\n",
            " Epoch:296, G_loss: 0.5367027521133423, D_loss:0.0006702576938550919\n",
            " Epoch:297, G_loss: 0.497558057308197, D_loss:0.001624289550818503\n",
            " Epoch:298, G_loss: 0.47546324133872986, D_loss:0.001036119821947068\n",
            " Epoch:299, G_loss: 0.6098861694335938, D_loss:0.0006012587182340212\n",
            " Epoch:300, G_loss: 0.49137791991233826, D_loss:0.0012832577340304852\n",
            " Epoch:301, G_loss: 0.5316066741943359, D_loss:0.0010166678694076836\n",
            " Epoch:302, G_loss: 0.544265866279602, D_loss:0.0005561414582189173\n",
            " Epoch:303, G_loss: 0.39606863260269165, D_loss:0.0012177485914435238\n",
            " Epoch:304, G_loss: 0.5270233154296875, D_loss:0.003325173514895141\n",
            " Epoch:305, G_loss: 0.4963436722755432, D_loss:0.0016588481375947595\n",
            " Epoch:306, G_loss: 0.44817686080932617, D_loss:0.0006145894003566355\n",
            " Epoch:307, G_loss: 0.4328300356864929, D_loss:0.002347291214391589\n",
            " Epoch:308, G_loss: 0.3201335072517395, D_loss:0.0022881473996676505\n",
            " Epoch:309, G_loss: 0.5484227538108826, D_loss:0.0015662740042898804\n",
            " Epoch:310, G_loss: 0.4108830690383911, D_loss:0.0017586927278898656\n",
            " Epoch:311, G_loss: 0.3662075400352478, D_loss:0.0018226116662845016\n",
            " Epoch:312, G_loss: 0.37367185950279236, D_loss:0.0010818191803991795\n",
            " Epoch:313, G_loss: 0.3992992639541626, D_loss:0.0034351733047515154\n",
            " Epoch:314, G_loss: 0.38998356461524963, D_loss:0.006259502610191703\n",
            " Epoch:315, G_loss: 0.2546809911727905, D_loss:0.002003111061640084\n",
            " Epoch:316, G_loss: 0.4257827401161194, D_loss:0.0007075973990140483\n",
            " Epoch:317, G_loss: 0.3318251371383667, D_loss:0.0016330776852555573\n",
            " Epoch:318, G_loss: 0.3548200726509094, D_loss:0.0024241164792329073\n",
            " Epoch:319, G_loss: 0.4831589460372925, D_loss:0.0027353178011253476\n",
            " Epoch:320, G_loss: 0.4978145360946655, D_loss:0.0011038550146622583\n",
            " Epoch:321, G_loss: 0.6844725608825684, D_loss:0.0010726828768383712\n",
            " Epoch:322, G_loss: 0.37408703565597534, D_loss:0.0022175927879288793\n",
            " Epoch:323, G_loss: 0.41843023896217346, D_loss:0.0029228576458990574\n",
            " Epoch:324, G_loss: 0.6184729933738708, D_loss:0.010626738076098263\n",
            " Epoch:325, G_loss: 0.5635881423950195, D_loss:0.00291598733747378\n",
            " Epoch:326, G_loss: 0.5582908391952515, D_loss:0.001549897133372724\n",
            " Epoch:327, G_loss: 0.4313521981239319, D_loss:0.0033704897068673745\n",
            " Epoch:328, G_loss: 0.464295893907547, D_loss:0.0033218595781363547\n",
            " Epoch:329, G_loss: 0.541782557964325, D_loss:0.002238385903183371\n",
            " Epoch:330, G_loss: 0.37497490644454956, D_loss:0.0016179183730855584\n",
            " Epoch:331, G_loss: 0.5697219371795654, D_loss:0.0038403359649237245\n",
            " Epoch:332, G_loss: 0.346656858921051, D_loss:0.005469849798828363\n",
            " Epoch:333, G_loss: 0.44352206587791443, D_loss:0.0032990656327456236\n",
            " Epoch:334, G_loss: 0.469907283782959, D_loss:0.0027008121833205223\n",
            " Epoch:335, G_loss: 0.4932955801486969, D_loss:0.0030375359347090125\n",
            " Epoch:336, G_loss: 0.5704102516174316, D_loss:0.003370764898136258\n",
            " Epoch:337, G_loss: 0.4896309971809387, D_loss:0.0023753854911774397\n",
            " Epoch:338, G_loss: 0.3925124704837799, D_loss:0.0018889694474637508\n",
            " Epoch:339, G_loss: 0.4905327260494232, D_loss:0.0029084490379318595\n",
            " Epoch:340, G_loss: 0.5536876916885376, D_loss:0.003967625380028039\n",
            " Epoch:341, G_loss: 0.528099536895752, D_loss:0.009098126785829663\n",
            " Epoch:342, G_loss: 0.4757154583930969, D_loss:0.004286434326786548\n",
            " Epoch:343, G_loss: 0.44325199723243713, D_loss:0.0022251012560445815\n",
            " Epoch:344, G_loss: 0.5915465354919434, D_loss:0.00692588638048619\n",
            " Epoch:345, G_loss: 0.4793887138366699, D_loss:0.011361544951796532\n",
            " Epoch:346, G_loss: 0.5969094038009644, D_loss:0.016961813787929714\n",
            " Epoch:347, G_loss: 0.7508025169372559, D_loss:0.011390247382223606\n",
            " Epoch:348, G_loss: 0.5367920994758606, D_loss:0.004054652061313391\n",
            " Epoch:349, G_loss: 0.5882255434989929, D_loss:0.009172773454338312\n",
            " Epoch:350, G_loss: 0.4265992343425751, D_loss:0.0024934320244938135\n",
            " Epoch:351, G_loss: 0.7148926854133606, D_loss:0.001544681319501251\n",
            " Epoch:352, G_loss: 0.6521041393280029, D_loss:0.009678392903879285\n",
            " Epoch:353, G_loss: 0.48496073484420776, D_loss:0.007539932848885655\n",
            " Epoch:354, G_loss: 0.6823948621749878, D_loss:0.03963373671285808\n",
            " Epoch:355, G_loss: 0.6700931191444397, D_loss:0.02869419939815998\n",
            " Epoch:356, G_loss: 0.8417186737060547, D_loss:0.022275874856859446\n",
            " Epoch:357, G_loss: 0.5300742387771606, D_loss:0.015991034917533398\n",
            " Epoch:358, G_loss: 0.872146487236023, D_loss:0.05862298235297203\n",
            " Epoch:359, G_loss: 0.9248595237731934, D_loss:0.02054636739194393\n",
            " Epoch:360, G_loss: 0.8903148174285889, D_loss:0.08908824250102043\n",
            " Epoch:361, G_loss: 0.8166501522064209, D_loss:0.029262397903949022\n",
            " Epoch:362, G_loss: 1.0248823165893555, D_loss:0.024452812038362026\n",
            " Epoch:363, G_loss: 0.715827465057373, D_loss:0.1362481340765953\n",
            " Epoch:364, G_loss: 1.4682831764221191, D_loss:0.11256846413016319\n",
            " Epoch:365, G_loss: 2.864008903503418, D_loss:0.8465281575918198\n",
            " Epoch:366, G_loss: 3.043869972229004, D_loss:0.15487164456862956\n",
            " Epoch:367, G_loss: 1.3787763118743896, D_loss:0.15396824036724865\n",
            " Epoch:368, G_loss: 0.8062255382537842, D_loss:0.05099862441420555\n",
            " Epoch:369, G_loss: 0.6466956734657288, D_loss:0.004918035410810262\n",
            " Epoch:370, G_loss: 0.4315239489078522, D_loss:0.007829636568203568\n",
            " Epoch:371, G_loss: 0.5752785205841064, D_loss:0.004570671357214451\n",
            " Epoch:372, G_loss: 0.7774257659912109, D_loss:0.0012814827205147594\n",
            " Epoch:373, G_loss: 0.4849286675453186, D_loss:0.002336599660338834\n",
            " Epoch:374, G_loss: 0.6329566240310669, D_loss:0.0036313383607193828\n",
            " Epoch:375, G_loss: 0.8700948357582092, D_loss:0.0024896214599721134\n",
            " Epoch:376, G_loss: 0.7428653240203857, D_loss:0.01701585139380768\n",
            " Epoch:377, G_loss: 0.7541518211364746, D_loss:0.0017672390677034855\n",
            " Epoch:378, G_loss: 0.5474384427070618, D_loss:0.0007684901938773692\n",
            " Epoch:379, G_loss: 0.5334678888320923, D_loss:0.006927495691343211\n",
            " Epoch:380, G_loss: 0.8049007058143616, D_loss:0.0005145105969859287\n",
            " Epoch:381, G_loss: 0.6642183661460876, D_loss:0.001512188173364848\n",
            " Epoch:382, G_loss: 0.5360299944877625, D_loss:0.0010281078575644642\n",
            " Epoch:383, G_loss: 0.7846223711967468, D_loss:0.0032723897020332515\n",
            " Epoch:384, G_loss: 0.4801155924797058, D_loss:0.0007052799046505243\n",
            " Epoch:385, G_loss: 0.807835578918457, D_loss:0.0020433444442460313\n",
            " Epoch:386, G_loss: 0.6105062961578369, D_loss:0.001249972774530761\n",
            " Epoch:387, G_loss: 0.6987537145614624, D_loss:0.000604966888204217\n",
            " Epoch:388, G_loss: 0.8875927329063416, D_loss:0.0005287039239192382\n",
            " Epoch:389, G_loss: 0.6352447271347046, D_loss:0.0012082391913281754\n",
            " Epoch:390, G_loss: 0.7318375706672668, D_loss:0.001030202314723283\n",
            " Epoch:391, G_loss: 0.66168212890625, D_loss:0.0013841143227182329\n",
            " Epoch:392, G_loss: 0.6539546251296997, D_loss:0.003110761084826663\n",
            " Epoch:393, G_loss: 0.6116047501564026, D_loss:0.0014569974737241864\n",
            " Epoch:394, G_loss: 0.42378368973731995, D_loss:0.0004611309850588441\n",
            " Epoch:395, G_loss: 0.5691001415252686, D_loss:0.0023658359423279762\n",
            " Epoch:396, G_loss: 0.5793691277503967, D_loss:0.001331272185780108\n",
            " Epoch:397, G_loss: 0.6079735159873962, D_loss:0.001907898928038776\n",
            " Epoch:398, G_loss: 0.37431174516677856, D_loss:0.0010804873891174793\n",
            " Epoch:399, G_loss: 0.4561153054237366, D_loss:0.0025759420241229236\n",
            " Epoch:400, G_loss: 0.43707573413848877, D_loss:0.007249646965647116\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}